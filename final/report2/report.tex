% Created 2017-03-01 Wed 20:38
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{listings}
\tolerance=1000
\usepackage{natbib}
\usepackage[linktocpage,pdfstartview=FitH,colorlinks,
linkcolor=blue,anchorcolor=blue,
citecolor=blue,filecolor=blue,menucolor=blue,urlcolor=blue]{hyperref}
\usepackage[margin=2cm]{geometry}
\pagenumbering{gobble}
\usepackage{wrapfig}
\usepackage{multicol}
\setlength\columnsep{20pt}
\author{Alejandro Rodr√≠guez Salamanca - r0650814 - Erasmus}
\date{}
\title{Character recognition with Hopfield networks}
\begin{document}

\maketitle

\begin{multicols}{2}

  \section*{Data preparation}
  Before starting to solve the tasks, the dataset to use must be
  constructed. For this exercise, we are asked to pre-pend the
  lowercase characters of our first and last name to the set of
  capital characters given in \texttt{prprob}. As my name is
  Alejandro Rodriguez, the characters I have to pre-pend are:
  \texttt{a, l, e, j, n, d, r, o, i, g, u, z}. Once this is done,
  it's time to solve the three proposed problems.

  \section*{Task 1: Retrieve the first 5 characters.}
  The first 5 characters of the alphabet are \texttt{a, l, e, j}
  and \texttt{n}, this is, the first five characters that appear in
  my first name and last name. After loading this characters, the
  pixels whose value is 0 must be changed to -1. This has to be done
  because Hopfield networks normally have units that take on values of
  1 or -1 as convention.

  % Image of the characters
  \begin{center}
    \includegraphics[height=\linewidth]{img/chars1}
  \end{center}

  With this five characters as attractor states, a Hopfield neural
  network is created. Then, three random pixels of each character
  are inverted (from 1 to -1 and vice versa). The objective of this
  inversion is to check if the network is able to reconstruct the
  distorsioned characters.

  \begin{center}
    \includegraphics[width=\linewidth]{img/net}
  \end{center}

  Executing the neural network with the noisy characters as input
  shows the following results:

  \begin{itemize}
  \item With a small number of steps: it can be seen that the
    characters are being attracted by the attractor states, but they still
    contain some noise.
    % Image with 2 steps
    \begin{center}
      \includegraphics[height=\linewidth]{img/recons1}
    \end{center}
  \item When the number of steps is increased: with just 5 steps, the
    characters are perfectly reconstructed.
    % Image with 5 steps
    \begin{center}
      \includegraphics[height=\linewidth]{img/recons2}
    \end{center}
  \end{itemize}

  Sporious patterns are local energy minima that are created during
  training, in addition to the intended target patterns. They are
  activity patterns that have not been explicitly embedded in the
  synaptic matrix, but are nonetheless stable. They are in other
  words "unwanted" attractor states that, by virtue of a finite
  overlap with the "wanted" attractor states, come about as a
  local minimum in the energy function. They can be composed of
  various combinations of the original patterns or simply the
  negation of any pattern in the original pattern set.

  There are three different types of spurious states:

  \begin{enumerate}
  \item For each stored pattern, its negation is also an attractor
  \item Any linear combination of an odd number of stored patterns.
    give rise to the so-called mixture states. Note that Spurious
    patterns that have an even number of states cannot exist, since they might sum up to zero.
  \item for large $p$, we get local minima that are not correlated to
    any linear combination of stored patterns.
  \end{enumerate}

  In this case, the existence of spurious patters is not noticeable.
  According to Hebb rule, a Hopfield network can store up to $0.138N$
  uncorrelated patterns, being $N$ the number of neurons. Replacing
  $N$ by its value in this problem, 35, gives that the uncorrelated
  patterns that can be stored is almost 5, which is the number of
  charaters used in this exercise.

  \section*{Task 2: Critical loading capacity}
  A Hopfield neural network exceed the loading capacity when the number of
  learned patterns over the number of units $p/N$ is greater than the critical
  capacity $\alpha \approx 0.138$.
  First, a number $p = 20$ characters is selected, and the error in the
  reconstruction is calculated as a function of the number of steps used
  to reconstruct the character.

  \begin{center}
    \includegraphics[width=\linewidth]{img/plot_error1}
  \end{center}

  The purpose of this plot is to see how the approximation becomes more
  accurate when the number of steps is increased. Now, let's see what
  happens when the number of characters varies while keeping the number
  of steps fixed at 15.

  \begin{center}
    \includegraphics[width=\linewidth]{img/plot_error2}
  \end{center}

  The critical capacity of a Hopfield network, as stated before, is $p/N$,
  where $p$ is the set of patterns to be memorized and $N$ the number
  of neurons. The number of neurons is determined by the number of pixels
  in each image. The dimension of each image is $7 \times 5$, which means that
  the number of neurons of the network is 35.

  Now, the critical capacity can be expressed as $p/35$.

  It can be seen in the previous plot that when the number of patterns
  $p$ has values below 15, there is no error in the reconstruction,
  but when it is increased, an error in the reconstruction appears.

  The loading capacity can be expressed then as $15/35 = 0.428$, which is
  much higher than the one obtained using Hebb-rule.

  This is an interesting phenomena. Nevertheless, it has an explanation. It
  could happen that the spurious states and the real attractors overlap, which
  means that the reconstruction of the character will be correct even if the
  pixel is reconstructed by an spurious state. Hebb rule states that it can be
  stored $0.138N$ uncorrelated patterns, which in this case would be
  $0.138 \times 35 \approx 5$ patterns, as aforementioned. It can be then concluded that
  the charaters are not uncorrelated patterns.    
  
  \section*{Task 3: Retrieve 25 characters}
  As it can be seen previously, Hopfield networks works well when the number
  of stored patterns is small, but when it increases, spurious states appears
  and the images cannot be reconstructed. For this reason, a different approach
  should be used.

  As we already know from previous assignments, a feedforward
  network with one hidden layer and enough neurons in the hidden layers,
  can fit any finite input-output mapping problem. But some changes
  should be done to use the data with a feedforward network.

  When using a feedforward network, a set of targets and data is needed.
  The data corresponds to the digits previously used, but it is neccessary to
  assign a class to each of the digits. It can be done creating an indentity matrix
  whose dimensions are the number of digits, 25 in this case.

  Different architectures and parameters where used, but in some cases the network where not
  able to identify correctly the characters. There are different techniques to improve
  the performance and accuracy of a neural network, but the simplest and most effective
  one is to generate more training data. As we are trying to idenfify noisy characters,
  is it possible to generate a set of noisy characters and append them to the original ones.
  Now, our network not only learns with the original characters, but also with noisy characters
  similar to the ones it will have to clasify later.

  As expected, the network learnt to clasify better the digits, but the results showed where
  not perfect. It happened that depending on which pixels were inverted, two different
  characters can become almost equal, as happens with G and C.

  At this point, the obvious idea is to generate more noisy characters and add them to our
  trainig data set. It could be an option, but we take the risk that our network does not
  learn to generalize.

  Can we do anything else to improve the accuracy of a neural network? Fortunately, yes, we can.
  Two common techniques to improve generalization and avoid overfitting are
  \textbf{retraining the neural network} and \textbf{using multiple neural networks}.

  The approach used consists of use multiple neural networks and average their outputs.
  This technique is really useful when the error in the classification is caused by noisy
  data or a small dataset, which is exactly this case.

  To solve this problem, ten feedfordward neural networks have been used, with
  50 neurons in the hidden layer, \texttt{tansig} transfer function for the output
  layer, and bayesian regularization backpropagation for training.

  \begin{center}
    \includegraphics[width=\linewidth]{img/net2}
  \end{center}
  
  The distorsioned data set of the first 25 characters can be seen in the following image:
  \begin{center}
    \includegraphics[width=\linewidth]{img/noisy25}
  \end{center}

  And finally, it is perfectly reconstructed:
  \begin{center}
    \includegraphics[width=\linewidth]{img/reconstructed25}
  \end{center}

  \section*{Conclusion}
  After solving the reconstruction of noisy digits problem using two different approaches,
  it's time to discuss the advantages and drawbacks of each one.

  Using Hopfield networks to reconstruct noisy characters is a good idea when the number
  of patters to store in the network, this is, the number of characters is small. Once
  the number of stored patterns grows, spurious states appers, and the accuracy of the
  network decreases. In fact, as it have been seen before, the maximum theoretical number
  of patters the network can store is $N \frac 2\times ln N $, being $N$ the number of neurons.
  However, Hopfield networks are fast when training, so if training speed is a bottleneck in
  our problem, using this type of network can be the solution.

  The second approach, feedfordward networks, is different from the first one in the sense that
  this network does not reconstruct the character, but identifies which one has been received as
  input. We have seen that this networks can fit any finite input-output mapping problem, which
  means that if the trainig data set is good enough and the number of neurons in the hidden layer
  is high, we can \textit{reconstruct} any set of characters, independently of its size, and if
  the training data set is small, other techniques can be used, as previously seen in this assignment.
  The main problem with this type of network is that it needs a lot of time for training, unlike
  Hopfield networks.

  It can be concluded that feedforward netwoks are more suitable for pattern recognition than
  Hopfield networks if the main concern is accuracy in the classification.
  
  %https://en.wikipedia.org/wiki/Hopfield_network
  %https://cogsci.stackexchange.com/questions/903/spurious-attractors-in-hopfield-networks
  %https://www.quora.com/What-are-spurious-states-in-Hopfield-networks
  % https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4546407/
  % http://www.doc.ic.ac.uk/~ae/papers/Hopfield-networks-15.pdf

\end{multicols}
\newpage

\section*{Appendix}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}

\lstset{language=Matlab,
    breaklines=true,
    commentstyle=\color{mygreen},
    numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
    numbersep=5pt,                   % how far the line-numbers are from the code
    numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
    rulecolor=\color{black},
    tabsize=2, 
  }          % Set your language (you can change the language for each code-block optionally)
  
  \subsection*{gen\_noisy\_chars.m}
  \lstinputlisting[frame=single]{../gen_noisy_chars.m}
  \subsection*{get\_alphabet.m}
  \lstinputlisting[frame=single]{../get_alphabet.m}
  \subsection*{show\_chars.m}
  \lstinputlisting[frame=single]{../show_chars.m}  
  \subsection*{ex2\_1.m}
  \lstinputlisting[frame=single]{../ex2_1.m}
  \subsection*{ex2\_2.m}
  \lstinputlisting[frame=single]{../ex2_2.m}
  \subsection*{ex2\_3.m}
  \lstinputlisting[frame=single]{../ex2_3.m}

\end{document}
